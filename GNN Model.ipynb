{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7666b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624b4337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv(\"/home/nilabjab/cancer_dependency_project_nilabja/cancer_dependency_project/Assignment/BioXAi/train.csv\")\n",
    "val_df = pd.read_csv(\"/home/nilabjab/cancer_dependency_project_nilabja/cancer_dependency_project/Assignment/BioXAi/val.csv\")\n",
    "test_df = pd.read_csv(\"/home/nilabjab/cancer_dependency_project_nilabja/cancer_dependency_project/Assignment/BioXAi/test.csv\")\n",
    "\n",
    "# Add a column to keep track of dataset source (optional but handy)\n",
    "train_df[\"split\"] = \"train\"\n",
    "val_df[\"split\"] = \"val\"\n",
    "test_df[\"split\"] = \"test\"\n",
    "\n",
    "# Merge all into a single DataFrame\n",
    "full_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caee013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "one_hot_df = pd.get_dummies(full_df['SAMPLE_DATA_TYPE'], prefix='DATA_TYPE').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751fb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([full_df, one_hot_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1596bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select concentration and inhibition columns\n",
    "conc_cols = [f'CONC{i}' for i in range(15)]\n",
    "inh_cols = [f'DATA{i}' for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9286c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4345d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create concentration and inhibition column names\n",
    "conc_cols = [f'CONC{i}' for i in range(15)]\n",
    "inh_cols = [f'DATA{i}' for i in range(15)]\n",
    "\n",
    "# Melt inhibition and concentration together\n",
    "long_df = full_df.melt(\n",
    "    id_vars=['canonical_smiles'] + list(one_hot_df.columns), \n",
    "    value_vars=inh_cols + conc_cols,\n",
    "    var_name='col_type', \n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "# Extract the type (CONC/DATA) and index (0â€“14)\n",
    "long_df['type'] = long_df['col_type'].str.extract(r'([A-Z]+)')\n",
    "long_df['index'] = long_df['col_type'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# Pivot to long format\n",
    "final_df = long_df.pivot_table(\n",
    "    index=['canonical_smiles', 'index'] + list(one_hot_df.columns), \n",
    "    columns='type', values='value'\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns\n",
    "final_df.rename(columns={'CONC': 'concentration', 'DATA': 'inhibition'}, inplace=True)\n",
    "\n",
    "# Apply log10 to concentration\n",
    "final_df['log_conc'] = np.log10(final_df['concentration'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f96c72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3692708/1060271207.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph_dataset = torch.load('/home/nilabjab/cancer_dependency_project_nilabja/cancer_dependency_project/Assignment/BioXAi/graph_dataset.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "graph_dataset = torch.load('/home/nilabjab/cancer_dependency_project_nilabja/cancer_dependency_project/Assignment/BioXAi/graph_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0231bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103518"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c34e6e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of SMILES that match the graph order\n",
    "smiles_list = full_df['canonical_smiles'].tolist()\n",
    "\n",
    "# Map SMILES to graph by position\n",
    "smiles_to_graph = dict(zip(smiles_list, graph_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fe87dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign to final_df\n",
    "final_df['graph'] = final_df['canonical_smiles'].map(smiles_to_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a21d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(columns='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9b63520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [(x, [tensor([5]), tensor([6]), tensor([6]), t...\n",
       "1         [(x, [tensor([5]), tensor([6]), tensor([6]), t...\n",
       "2         [(x, [tensor([5]), tensor([6]), tensor([6]), t...\n",
       "3         [(x, [tensor([5]), tensor([6]), tensor([6]), t...\n",
       "4         [(x, [tensor([5]), tensor([6]), tensor([6]), t...\n",
       "                                ...                        \n",
       "974535    [(x, [tensor([8]), tensor([34]), tensor([8]), ...\n",
       "974536    [(x, [tensor([8]), tensor([34]), tensor([8]), ...\n",
       "974537    [(x, [tensor([8]), tensor([34]), tensor([8]), ...\n",
       "974538    [(x, [tensor([8]), tensor([34]), tensor([8]), ...\n",
       "974539    [(x, [tensor([8]), tensor([34]), tensor([8]), ...\n",
       "Name: graph, Length: 974540, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea41b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb3ac59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. Helper function to parse your graph string ---\n",
    "# This is a placeholder. You'll need to adapt this based on the actual\n",
    "# structure of your 'graph0', 'graph1', etc. columns.\n",
    "# For this example, I'll assume it means you have node features (x)\n",
    "# and edge_index.\n",
    "def parse_graph_string(graph_str):\n",
    "    # Example: \"[(x, [tensor([5]), tensor([6]), ...]), (edge_index, ...)]\"\n",
    "    # This is highly dependent on your actual string format.\n",
    "    # You'll need to implement robust parsing here.\n",
    "    # For now, let's assume a simplified scenario where you can extract\n",
    "    # node features and edge connectivity directly or from SMILES.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c43dc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Data ---\n",
    "\n",
    "# Let's assume 'inhibition' is your target variable (response)\n",
    "TARGET_COLUMN = 'inhibition'\n",
    "\n",
    "# --- 2. SMILES to Graph Conversion (using RDKit) ---\n",
    "# This function converts a SMILES string to a PyG Data object.\n",
    "# Node features can be atomic properties. Edge features can represent bond types.\n",
    "\n",
    "def smiles_to_pyg_graph(smiles, other_features, y):\n",
    "    \"\"\"\n",
    "    Converts a SMILES string and other features to a PyG Data object.\n",
    "\n",
    "    Args:\n",
    "        smiles (str): The SMILES string of the molecule.\n",
    "        other_features (torch.Tensor): Tensor of other tabular features.\n",
    "        y (float): The target variable.\n",
    "\n",
    "    Returns:\n",
    "        torch_geometric.data.Data: A PyG Data object, or None if conversion fails.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    # --- Node Features (Atom Features) ---\n",
    "    # Example: Atomic number, formal charge, hybridization, aromaticity, etc.\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features.append([\n",
    "            atom.GetAtomicNum(),\n",
    "            atom.GetFormalCharge(),\n",
    "            float(atom.GetHybridization()), # Convert HybridizationType to float\n",
    "            int(atom.GetIsAromatic()),\n",
    "            atom.GetDegree(), # Number of explicit and implicit Hs\n",
    "            atom.GetTotalNumHs(),\n",
    "            # Add more features as needed\n",
    "        ])\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "\n",
    "    if x.shape[0] == 0: # No atoms in molecule (e.g. empty smiles)\n",
    "        return None\n",
    "\n",
    "    # --- Edge Index (Adjacency List) and Edge Features (Bond Features) ---\n",
    "    edge_indices = []\n",
    "    edge_attrs = []\n",
    "    for bond in mol.GetBonds():\n",
    "        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_indices.append((start, end))\n",
    "        edge_indices.append((end, start)) # Add edges in both directions for undirected graphs\n",
    "\n",
    "        # Example: Bond type\n",
    "        bond_type = bond.GetBondTypeAsDouble() # GetBondType() returns BondType, convert to numeric\n",
    "        edge_attrs.append([bond_type])\n",
    "        edge_attrs.append([bond_type])\n",
    "\n",
    "    if not edge_indices: # If no bonds, create a self-loop for each node or handle as needed\n",
    "        if x.shape[0] > 0:\n",
    "            edge_index = torch.empty((2,0), dtype=torch.long) # No edges\n",
    "            edge_attr = torch.empty((0,1), dtype=torch.float) # No edge features\n",
    "        else: # no nodes, no edges\n",
    "             return None # Or handle this case appropriately\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                other_features=other_features.unsqueeze(0), # Add batch dimension for concatenation later\n",
    "                y=torch.tensor([y], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8efb6a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>DATA_TYPE_agonist1</th>\n",
       "      <th>DATA_TYPE_agonist2</th>\n",
       "      <th>DATA_TYPE_agonist3</th>\n",
       "      <th>DATA_TYPE_antagonist1</th>\n",
       "      <th>DATA_TYPE_antagonist2</th>\n",
       "      <th>DATA_TYPE_antagonist3</th>\n",
       "      <th>DATA_TYPE_cell_red</th>\n",
       "      <th>DATA_TYPE_viability1</th>\n",
       "      <th>DATA_TYPE_viability2</th>\n",
       "      <th>DATA_TYPE_viability3</th>\n",
       "      <th>concentration</th>\n",
       "      <th>inhibition</th>\n",
       "      <th>log_conc</th>\n",
       "      <th>graph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B(C(CC(C)C)NC(=O)C(CC1=CC=CC=C1)NC(=O)C2=NC=CN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.899000e-10</td>\n",
       "      <td>-2.509096</td>\n",
       "      <td>-9.229222</td>\n",
       "      <td>Data(x=[28, 1], edge_index=[2, 58])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B(C(CC(C)C)NC(=O)C(CC1=CC=CC=C1)NC(=O)C2=NC=CN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.899000e-10</td>\n",
       "      <td>-17.185048</td>\n",
       "      <td>-9.229222</td>\n",
       "      <td>Data(x=[28, 1], edge_index=[2, 58])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B(C(CC(C)C)NC(=O)C(CC1=CC=CC=C1)NC(=O)C2=NC=CN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.899000e-10</td>\n",
       "      <td>-3.018927</td>\n",
       "      <td>-9.229222</td>\n",
       "      <td>Data(x=[28, 1], edge_index=[2, 58])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B(C(CC(C)C)NC(=O)C(CC1=CC=CC=C1)NC(=O)C2=NC=CN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.373000e-08</td>\n",
       "      <td>0.356070</td>\n",
       "      <td>-7.132356</td>\n",
       "      <td>Data(x=[28, 1], edge_index=[2, 58])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B(C(CC(C)C)NC(=O)C(CC1=CC=CC=C1)NC(=O)C2=NC=CN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.899000e-10</td>\n",
       "      <td>-1.710692</td>\n",
       "      <td>-9.229222</td>\n",
       "      <td>Data(x=[28, 1], edge_index=[2, 58])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974535</th>\n",
       "      <td>[O-][Se](=O)[O-].[Na+].[Na+]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.326000e-05</td>\n",
       "      <td>0.382924</td>\n",
       "      <td>-4.633390</td>\n",
       "      <td>Data(x=[6, 1], edge_index=[2, 6])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974536</th>\n",
       "      <td>[O-][Se](=O)[O-].[Na+].[Na+]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.326000e-05</td>\n",
       "      <td>0.518981</td>\n",
       "      <td>-4.633390</td>\n",
       "      <td>Data(x=[6, 1], edge_index=[2, 6])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974537</th>\n",
       "      <td>[O-][Se](=O)[O-].[Na+].[Na+]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.326000e-05</td>\n",
       "      <td>-0.451559</td>\n",
       "      <td>-4.633390</td>\n",
       "      <td>Data(x=[6, 1], edge_index=[2, 6])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974538</th>\n",
       "      <td>[O-][Se](=O)[O-].[Na+].[Na+]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.326000e-05</td>\n",
       "      <td>0.590535</td>\n",
       "      <td>-4.633390</td>\n",
       "      <td>Data(x=[6, 1], edge_index=[2, 6])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974539</th>\n",
       "      <td>[O-][Se](=O)[O-].[Na+].[Na+]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.326000e-05</td>\n",
       "      <td>0.266894</td>\n",
       "      <td>-4.633390</td>\n",
       "      <td>Data(x=[6, 1], edge_index=[2, 6])</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>974540 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         canonical_smiles  DATA_TYPE_agonist1  \\\n",
       "0       B(C(CC(C)C)NC(=O)C(CC1=CC=CC=C1)NC(=O)C2=NC=CN...                   0   \n",
       "1       B(C(CC(C)C)NC(=O)C(CC1=CC=CC=C1)NC(=O)C2=NC=CN...                   0   \n",
       "2       B(C(CC(C)C)NC(=O)C(CC1=CC=CC=C1)NC(=O)C2=NC=CN...                   0   \n",
       "3       B(C(CC(C)C)NC(=O)C(CC1=CC=CC=C1)NC(=O)C2=NC=CN...                   0   \n",
       "4       B(C(CC(C)C)NC(=O)C(CC1=CC=CC=C1)NC(=O)C2=NC=CN...                   0   \n",
       "...                                                   ...                 ...   \n",
       "974535                       [O-][Se](=O)[O-].[Na+].[Na+]                   0   \n",
       "974536                       [O-][Se](=O)[O-].[Na+].[Na+]                   0   \n",
       "974537                       [O-][Se](=O)[O-].[Na+].[Na+]                   0   \n",
       "974538                       [O-][Se](=O)[O-].[Na+].[Na+]                   0   \n",
       "974539                       [O-][Se](=O)[O-].[Na+].[Na+]                   1   \n",
       "\n",
       "        DATA_TYPE_agonist2  DATA_TYPE_agonist3  DATA_TYPE_antagonist1  \\\n",
       "0                        0                   0                      0   \n",
       "1                        0                   0                      0   \n",
       "2                        0                   0                      0   \n",
       "3                        0                   0                      0   \n",
       "4                        0                   0                      0   \n",
       "...                    ...                 ...                    ...   \n",
       "974535                   0                   0                      0   \n",
       "974536                   0                   0                      1   \n",
       "974537                   0                   1                      0   \n",
       "974538                   1                   0                      0   \n",
       "974539                   0                   0                      0   \n",
       "\n",
       "        DATA_TYPE_antagonist2  DATA_TYPE_antagonist3  DATA_TYPE_cell_red  \\\n",
       "0                           0                      0                   0   \n",
       "1                           0                      0                   0   \n",
       "2                           0                      0                   0   \n",
       "3                           0                      0                   1   \n",
       "4                           0                      1                   0   \n",
       "...                       ...                    ...                 ...   \n",
       "974535                      1                      0                   0   \n",
       "974536                      0                      0                   0   \n",
       "974537                      0                      0                   0   \n",
       "974538                      0                      0                   0   \n",
       "974539                      0                      0                   0   \n",
       "\n",
       "        DATA_TYPE_viability1  DATA_TYPE_viability2  DATA_TYPE_viability3  \\\n",
       "0                          0                     0                     1   \n",
       "1                          0                     1                     0   \n",
       "2                          1                     0                     0   \n",
       "3                          0                     0                     0   \n",
       "4                          0                     0                     0   \n",
       "...                      ...                   ...                   ...   \n",
       "974535                     0                     0                     0   \n",
       "974536                     0                     0                     0   \n",
       "974537                     0                     0                     0   \n",
       "974538                     0                     0                     0   \n",
       "974539                     0                     0                     0   \n",
       "\n",
       "        concentration  inhibition  log_conc  \\\n",
       "0        5.899000e-10   -2.509096 -9.229222   \n",
       "1        5.899000e-10  -17.185048 -9.229222   \n",
       "2        5.899000e-10   -3.018927 -9.229222   \n",
       "3        7.373000e-08    0.356070 -7.132356   \n",
       "4        5.899000e-10   -1.710692 -9.229222   \n",
       "...               ...         ...       ...   \n",
       "974535   2.326000e-05    0.382924 -4.633390   \n",
       "974536   2.326000e-05    0.518981 -4.633390   \n",
       "974537   2.326000e-05   -0.451559 -4.633390   \n",
       "974538   2.326000e-05    0.590535 -4.633390   \n",
       "974539   2.326000e-05    0.266894 -4.633390   \n",
       "\n",
       "                                      graph  \n",
       "0       Data(x=[28, 1], edge_index=[2, 58])  \n",
       "1       Data(x=[28, 1], edge_index=[2, 58])  \n",
       "2       Data(x=[28, 1], edge_index=[2, 58])  \n",
       "3       Data(x=[28, 1], edge_index=[2, 58])  \n",
       "4       Data(x=[28, 1], edge_index=[2, 58])  \n",
       "...                                     ...  \n",
       "974535    Data(x=[6, 1], edge_index=[2, 6])  \n",
       "974536    Data(x=[6, 1], edge_index=[2, 6])  \n",
       "974537    Data(x=[6, 1], edge_index=[2, 6])  \n",
       "974538    Data(x=[6, 1], edge_index=[2, 6])  \n",
       "974539    Data(x=[6, 1], edge_index=[2, 6])  \n",
       "\n",
       "[974540 rows x 15 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab0b2b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:11:35] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 974540 molecules to graph data.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Prepare Data for PyG ---\n",
    "data_list = []\n",
    "# Define which columns are 'other_features' (non-graph, non-target features)\n",
    "# These are your one-hot encoded columns and log_conc\n",
    "other_feature_cols = [\n",
    "    'DATA_TYPE_agonist1', 'DATA_TYPE_agonist2', 'DATA_TYPE_agonist3',\n",
    "    'DATA_TYPE_antagonist1', 'DATA_TYPE_antagonist2', 'DATA_TYPE_antagonist3',\n",
    "    'DATA_TYPE_cell_red', 'DATA_TYPE_viability1', 'DATA_TYPE_viability2', 'DATA_TYPE_viability3',\n",
    "    'log_conc' # 'concentration' could also be used, or log_conc if it's more stable\n",
    "]\n",
    "\n",
    "for index, row in final_df.iterrows():\n",
    "    smiles = row['canonical_smiles']\n",
    "    y = row[TARGET_COLUMN]\n",
    "    # Extract other features and convert to a tensor\n",
    "    other_feats_values = row[other_feature_cols].values.astype(np.float32)\n",
    "    other_features_tensor = torch.tensor(other_feats_values, dtype=torch.float)\n",
    "\n",
    "    # If you have pre-computed graph features in 'graph0', 'graph1', etc.\n",
    "    # you would parse them here instead of using smiles_to_pyg_graph directly,\n",
    "    # or use them to augment the graph features from RDKit.\n",
    "    # For this example, we regenerate from SMILES.\n",
    "    # graph_str = row['graph0'] # or graph1 etc. based on your logic\n",
    "    # parsed_x, parsed_edge_index = parse_graph_string(graph_str) # Implement this!\n",
    "\n",
    "    graph_data = smiles_to_pyg_graph(smiles, other_features_tensor, y)\n",
    "    if graph_data:\n",
    "        data_list.append(graph_data)\n",
    "\n",
    "print(f\"Successfully converted {len(data_list)} molecules to graph data.\")\n",
    "if not data_list:\n",
    "    raise ValueError(\"No graph data could be created. Check SMILES strings and feature extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "471229c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 779632\n",
      "Number of validation samples: 97454\n",
      "Number of test samples: 97454\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Split Data ---\n",
    "train_data, temp_data = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(val_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")\n",
    "\n",
    "# At this point, each element in train_data, val_data, test_data is a PyG 'Data' object.\n",
    "# Each 'Data' object contains:\n",
    "# - data.x: Node features (from atoms)\n",
    "# - data.edge_index: Graph connectivity\n",
    "# - data.edge_attr: Edge features (from bonds)\n",
    "# - data.other_features: Your one-hot encoded data and log_conc\n",
    "# - data.y: The target variable ('inhibition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27399997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader # Corrected import\n",
    "\n",
    "# Batch size\n",
    "batch_size = 64 # Keep it small for this dummy data example\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example of a batch:\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     print(batch.x.shape)\n",
    "#     print(batch.edge_index.shape)\n",
    "#     print(batch.other_features.shape)\n",
    "#     print(batch.y.shape)\n",
    "#     print(batch.batch) # This tensor maps each node to its respective graph in the batch\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35877e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNPredictor(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(6, 128)\n",
      "    (1-2): 2 x GCNConv(128, 128)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0-2): 3 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=139, out_features=256, bias=True)\n",
      "  (fc_bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc_bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv, GINConv # Example GNN layers\n",
    "\n",
    "class GNNPredictor(nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, other_feature_dim, hidden_dim, output_dim=1, n_gnn_layers=3, dropout_rate=0.4):\n",
    "        super(GNNPredictor, self).__init__()\n",
    "        self.n_gnn_layers = n_gnn_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # GNN Layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList() # Batch Norm for GNN layers\n",
    "\n",
    "        # First GNN layer\n",
    "        self.convs.append(GCNConv(node_feature_dim, hidden_dim)) # Or GATConv, GINConv etc.\n",
    "        self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Subsequent GNN layers\n",
    "        for _ in range(n_gnn_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Calculate combined feature dimension after GNN and concatenation\n",
    "        combined_feature_dim = hidden_dim + other_feature_dim # After global pooling\n",
    "\n",
    "        # Fully Connected Layers for combined features\n",
    "        self.fc1 = nn.Linear(combined_feature_dim, hidden_dim * 2)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc_bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, other_features, batch = data.x, data.edge_index, data.edge_attr, data.other_features, data.batch\n",
    "\n",
    "        # GNN part\n",
    "        for i in range(self.n_gnn_layers):\n",
    "            x = self.convs[i](x, edge_index, edge_weight=None) # Add edge_attr if layer supports it, e.g. GATConv can use edge_attr in attention\n",
    "                                                               # GCNConv typically doesn't use edge_attr directly in its basic form.\n",
    "                                                               # If using edge_attr, ensure GCNConv is modified or use a different conv layer.\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # Global pooling (e.g., mean pooling) to get a graph-level embedding\n",
    "        graph_embedding = global_mean_pool(x, batch) # `batch` vector is crucial here\n",
    "\n",
    "        # Ensure other_features has the correct shape for concatenation\n",
    "        # other_features is already [batch_size, num_other_features] from DataLoader\n",
    "        # graph_embedding will be [batch_size, hidden_dim]\n",
    "\n",
    "        # Concatenate graph embedding with other features\n",
    "        combined_features = torch.cat([graph_embedding, other_features], dim=1)\n",
    "\n",
    "        # Fully Connected Layers part\n",
    "        out = self.fc1(combined_features)\n",
    "        out = self.fc_bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc_bn2(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# --- Determine feature dimensions ---\n",
    "# Get a sample data to infer dimensions (important!)\n",
    "if not train_data:\n",
    "    raise ValueError(\"Training data is empty. Cannot infer feature dimensions.\")\n",
    "\n",
    "sample_data = train_data[0]\n",
    "node_dim = sample_data.x.shape[1]\n",
    "edge_dim = sample_data.edge_attr.shape[1] if sample_data.edge_attr is not None and sample_data.edge_attr.ndim > 1 else 0 # Handle no edge features\n",
    "other_dim = sample_data.other_features.shape[1]\n",
    "\n",
    "hidden_channels = 128 # Hyperparameter\n",
    "num_gnn_layers = 3    # Hyperparameter\n",
    "dropout = 0.3        # Hyperparameter\n",
    "\n",
    "model = GNNPredictor(node_feature_dim=node_dim,\n",
    "                     edge_feature_dim=edge_dim, # Pass if your conv layer uses it\n",
    "                     other_feature_dim=other_dim,\n",
    "                     hidden_dim=hidden_channels,\n",
    "                     output_dim=1, # Predicting a single value (inhibition)\n",
    "                     n_gnn_layers=num_gnn_layers,\n",
    "                     dropout_rate=dropout)\n",
    "\n",
    "print(model)\n",
    "# Test a forward pass with a batch (ensure dimensions match)\n",
    "# for batch_data in train_loader:\n",
    "#     try:\n",
    "#         output = model(batch_data)\n",
    "#         print(\"Forward pass successful. Output shape:\", output.shape)\n",
    "#         print(\"Target shape:\", batch_data.y.shape)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during forward pass: {e}\")\n",
    "#         print(\"Batch data details:\")\n",
    "#         print(f\"  x shape: {batch_data.x.shape}, type: {batch_data.x.dtype}\")\n",
    "#         print(f\"  edge_index shape: {batch_data.edge_index.shape}, type: {batch_data.edge_index.dtype}\")\n",
    "#         if batch_data.edge_attr is not None:\n",
    "#             print(f\"  edge_attr shape: {batch_data.edge_attr.shape}, type: {batch_data.edge_attr.dtype}\")\n",
    "#         else:\n",
    "#             print(f\"  edge_attr is None\")\n",
    "#         print(f\"  other_features shape: {batch_data.other_features.shape}, type: {batch_data.other_features.dtype}\")\n",
    "#         print(f\"  batch tensor: {batch_data.batch.shape}, type: {batch_data.batch.dtype}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nilabjab/miniconda3/envs/pyenv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training...\n",
      "Epoch 001: Train Loss: 172.4209, Val Loss: 153.7553, LR: 0.001000\n",
      "Saved new best model with Val Loss: 153.7553\n",
      "Epoch 002: Train Loss: 160.1925, Val Loss: 146.3849, LR: 0.001000\n",
      "Saved new best model with Val Loss: 146.3849\n",
      "Epoch 003: Train Loss: 153.5186, Val Loss: 142.1578, LR: 0.001000\n",
      "Saved new best model with Val Loss: 142.1578\n",
      "Epoch 004: Train Loss: 149.8162, Val Loss: 132.9507, LR: 0.001000\n",
      "Saved new best model with Val Loss: 132.9507\n",
      "Epoch 005: Train Loss: 145.0792, Val Loss: 128.1507, LR: 0.001000\n",
      "Saved new best model with Val Loss: 128.1507\n",
      "Epoch 006: Train Loss: 141.7345, Val Loss: 124.5425, LR: 0.001000\n",
      "Saved new best model with Val Loss: 124.5425\n",
      "Epoch 007: Train Loss: 139.1298, Val Loss: 122.9401, LR: 0.001000\n",
      "Saved new best model with Val Loss: 122.9401\n",
      "Epoch 008: Train Loss: 136.5693, Val Loss: 119.2809, LR: 0.001000\n",
      "Saved new best model with Val Loss: 119.2809\n",
      "Epoch 009: Train Loss: 134.8978, Val Loss: 118.0033, LR: 0.001000\n",
      "Saved new best model with Val Loss: 118.0033\n",
      "Epoch 010: Train Loss: 132.5347, Val Loss: 111.7067, LR: 0.001000\n",
      "Saved new best model with Val Loss: 111.7067\n",
      "Epoch 011: Train Loss: 131.0791, Val Loss: 107.0126, LR: 0.001000\n",
      "Saved new best model with Val Loss: 107.0126\n",
      "Epoch 012: Train Loss: 129.6869, Val Loss: 107.4858, LR: 0.001000\n",
      "Epoch 013: Train Loss: 127.5630, Val Loss: 103.5133, LR: 0.001000\n",
      "Saved new best model with Val Loss: 103.5133\n",
      "Epoch 014: Train Loss: 126.7045, Val Loss: 103.3965, LR: 0.001000\n",
      "Saved new best model with Val Loss: 103.3965\n",
      "Epoch 015: Train Loss: 124.9199, Val Loss: 99.6305, LR: 0.001000\n",
      "Saved new best model with Val Loss: 99.6305\n",
      "Epoch 016: Train Loss: 124.3849, Val Loss: 106.0539, LR: 0.001000\n",
      "Epoch 017: Train Loss: 122.9258, Val Loss: 99.7137, LR: 0.001000\n",
      "Epoch 018: Train Loss: 122.1057, Val Loss: 100.7197, LR: 0.001000\n",
      "Epoch 019: Train Loss: 121.2359, Val Loss: 97.2027, LR: 0.001000\n",
      "Saved new best model with Val Loss: 97.2027\n",
      "Epoch 020: Train Loss: 120.8595, Val Loss: 98.1741, LR: 0.001000\n",
      "Epoch 021: Train Loss: 119.9783, Val Loss: 92.9987, LR: 0.001000\n",
      "Saved new best model with Val Loss: 92.9987\n",
      "Epoch 022: Train Loss: 118.7816, Val Loss: 95.4799, LR: 0.001000\n",
      "Epoch 023: Train Loss: 119.0797, Val Loss: 93.6653, LR: 0.001000\n",
      "Epoch 024: Train Loss: 117.3879, Val Loss: 93.1243, LR: 0.001000\n",
      "Epoch 025: Train Loss: 116.4842, Val Loss: 91.5202, LR: 0.001000\n",
      "Saved new best model with Val Loss: 91.5202\n",
      "Epoch 026: Train Loss: 116.8793, Val Loss: 98.2761, LR: 0.001000\n",
      "Epoch 027: Train Loss: 116.3056, Val Loss: 88.2089, LR: 0.001000\n",
      "Saved new best model with Val Loss: 88.2089\n",
      "Epoch 028: Train Loss: 115.5585, Val Loss: 91.0840, LR: 0.001000\n",
      "Epoch 029: Train Loss: 115.1514, Val Loss: 91.9783, LR: 0.001000\n",
      "Epoch 030: Train Loss: 114.9237, Val Loss: 88.2177, LR: 0.001000\n",
      "Epoch 031: Train Loss: 113.3607, Val Loss: 91.5622, LR: 0.001000\n",
      "Epoch 032: Train Loss: 112.8158, Val Loss: 94.2429, LR: 0.001000\n",
      "Epoch 033: Train Loss: 112.6392, Val Loss: 84.6232, LR: 0.001000\n",
      "Saved new best model with Val Loss: 84.6232\n",
      "Epoch 034: Train Loss: 112.8261, Val Loss: 87.2661, LR: 0.001000\n",
      "Epoch 035: Train Loss: 111.9515, Val Loss: 85.2777, LR: 0.001000\n",
      "Epoch 036: Train Loss: 111.3573, Val Loss: 81.5436, LR: 0.001000\n",
      "Saved new best model with Val Loss: 81.5436\n",
      "Epoch 037: Train Loss: 111.3411, Val Loss: 85.7972, LR: 0.001000\n",
      "Epoch 038: Train Loss: 110.0279, Val Loss: 88.1595, LR: 0.001000\n",
      "Epoch 039: Train Loss: 110.4458, Val Loss: 91.5434, LR: 0.001000\n",
      "Epoch 040: Train Loss: 110.5752, Val Loss: 86.1962, LR: 0.001000\n",
      "Epoch 041: Train Loss: 109.4938, Val Loss: 84.2423, LR: 0.001000\n",
      "Epoch 042: Train Loss: 109.6301, Val Loss: 85.3105, LR: 0.001000\n",
      "Epoch 043: Train Loss: 109.1569, Val Loss: 83.7054, LR: 0.001000\n",
      "Epoch 044: Train Loss: 108.8937, Val Loss: 81.5865, LR: 0.001000\n",
      "Epoch 045: Train Loss: 108.6963, Val Loss: 80.8881, LR: 0.001000\n",
      "Saved new best model with Val Loss: 80.8881\n",
      "Epoch 046: Train Loss: 108.4424, Val Loss: 79.0435, LR: 0.001000\n",
      "Saved new best model with Val Loss: 79.0435\n",
      "Epoch 047: Train Loss: 108.1187, Val Loss: 79.9601, LR: 0.001000\n",
      "Epoch 048: Train Loss: 108.8821, Val Loss: 82.3184, LR: 0.001000\n",
      "Epoch 049: Train Loss: 108.0110, Val Loss: 81.1049, LR: 0.001000\n",
      "Epoch 050: Train Loss: 107.1609, Val Loss: 90.4724, LR: 0.001000\n",
      "Epoch 051: Train Loss: 106.8445, Val Loss: 79.3133, LR: 0.001000\n",
      "Epoch 052: Train Loss: 106.7415, Val Loss: 80.5709, LR: 0.001000\n",
      "Epoch 053: Train Loss: 106.8786, Val Loss: 79.2796, LR: 0.001000\n",
      "Epoch 054: Train Loss: 106.3026, Val Loss: 81.7588, LR: 0.001000\n",
      "Epoch 055: Train Loss: 106.5086, Val Loss: 79.1208, LR: 0.001000\n",
      "Epoch 056: Train Loss: 106.1308, Val Loss: 80.5587, LR: 0.001000\n",
      "Epoch 057: Train Loss: 106.3092, Val Loss: 79.5931, LR: 0.000500\n",
      "Epoch 058: Train Loss: 101.3047, Val Loss: 75.9592, LR: 0.000500\n",
      "Saved new best model with Val Loss: 75.9592\n",
      "Epoch 059: Train Loss: 100.3650, Val Loss: 77.7351, LR: 0.000500\n",
      "Epoch 060: Train Loss: 99.4819, Val Loss: 74.0549, LR: 0.000500\n",
      "Saved new best model with Val Loss: 74.0549\n",
      "Epoch 061: Train Loss: 100.0979, Val Loss: 76.3110, LR: 0.000500\n",
      "Epoch 062: Train Loss: 99.0316, Val Loss: 75.5884, LR: 0.000500\n",
      "Epoch 063: Train Loss: 99.5425, Val Loss: 78.9964, LR: 0.000500\n",
      "Epoch 064: Train Loss: 98.6453, Val Loss: 73.7859, LR: 0.000500\n",
      "Saved new best model with Val Loss: 73.7859\n",
      "Epoch 065: Train Loss: 98.4832, Val Loss: 74.6134, LR: 0.000500\n",
      "Epoch 066: Train Loss: 99.0807, Val Loss: 71.6824, LR: 0.000500\n",
      "Saved new best model with Val Loss: 71.6824\n",
      "Epoch 067: Train Loss: 98.0035, Val Loss: 71.8593, LR: 0.000500\n",
      "Epoch 068: Train Loss: 97.8845, Val Loss: 74.5411, LR: 0.000500\n",
      "Epoch 069: Train Loss: 97.8955, Val Loss: 71.8527, LR: 0.000500\n",
      "Epoch 070: Train Loss: 97.9801, Val Loss: 74.5681, LR: 0.000500\n",
      "Epoch 071: Train Loss: 97.9487, Val Loss: 74.4113, LR: 0.000500\n",
      "Epoch 072: Train Loss: 98.3613, Val Loss: 74.5382, LR: 0.000500\n",
      "Epoch 073: Train Loss: 97.4402, Val Loss: 70.3023, LR: 0.000500\n",
      "Saved new best model with Val Loss: 70.3023\n",
      "Epoch 074: Train Loss: 97.8982, Val Loss: 69.7015, LR: 0.000500\n",
      "Saved new best model with Val Loss: 69.7015\n",
      "Epoch 075: Train Loss: 97.0469, Val Loss: 73.5775, LR: 0.000500\n",
      "Epoch 076: Train Loss: 96.9859, Val Loss: 76.4634, LR: 0.000500\n",
      "Epoch 077: Train Loss: 96.3880, Val Loss: 77.9372, LR: 0.000500\n",
      "Epoch 078: Train Loss: 96.7076, Val Loss: 74.7461, LR: 0.000500\n",
      "Epoch 079: Train Loss: 96.6004, Val Loss: 72.1288, LR: 0.000500\n",
      "Epoch 080: Train Loss: 96.3074, Val Loss: 75.9901, LR: 0.000500\n",
      "Epoch 081: Train Loss: 97.0991, Val Loss: 72.1213, LR: 0.000500\n",
      "Epoch 082: Train Loss: 96.8801, Val Loss: 73.4756, LR: 0.000500\n",
      "Epoch 083: Train Loss: 95.8944, Val Loss: 75.2784, LR: 0.000500\n",
      "Epoch 084: Train Loss: 96.2075, Val Loss: 70.3397, LR: 0.000500\n",
      "Epoch 085: Train Loss: 95.9785, Val Loss: 70.4522, LR: 0.000250\n",
      "Epoch 086: Train Loss: 94.7633, Val Loss: 71.8255, LR: 0.000250\n",
      "Epoch 087: Train Loss: 93.0164, Val Loss: 72.6775, LR: 0.000250\n",
      "Epoch 088: Train Loss: 93.4322, Val Loss: 69.1042, LR: 0.000250\n",
      "Saved new best model with Val Loss: 69.1042\n",
      "Epoch 089: Train Loss: 92.6880, Val Loss: 69.2116, LR: 0.000250\n",
      "Epoch 090: Train Loss: 92.2888, Val Loss: 71.3127, LR: 0.000250\n",
      "Epoch 091: Train Loss: 92.4423, Val Loss: 68.5407, LR: 0.000250\n",
      "Saved new best model with Val Loss: 68.5407\n",
      "Epoch 092: Train Loss: 91.9923, Val Loss: 69.5408, LR: 0.000250\n",
      "Epoch 093: Train Loss: 91.8681, Val Loss: 71.3379, LR: 0.000250\n",
      "Epoch 094: Train Loss: 92.7521, Val Loss: 68.2356, LR: 0.000250\n",
      "Saved new best model with Val Loss: 68.2356\n",
      "Epoch 095: Train Loss: 92.3457, Val Loss: 67.5303, LR: 0.000250\n",
      "Saved new best model with Val Loss: 67.5303\n",
      "Epoch 096: Train Loss: 91.5305, Val Loss: 71.7706, LR: 0.000250\n",
      "Epoch 097: Train Loss: 91.7010, Val Loss: 69.4457, LR: 0.000250\n",
      "Epoch 098: Train Loss: 92.3673, Val Loss: 71.9437, LR: 0.000250\n",
      "Epoch 099: Train Loss: 91.4202, Val Loss: 68.7931, LR: 0.000250\n",
      "Epoch 100: Train Loss: 91.9547, Val Loss: 67.4703, LR: 0.000250\n",
      "Saved new best model with Val Loss: 67.4703\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function (Mean Squared Error for regression)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4) # Added weight decay for regularization\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10, verbose=True) # Learning rate scheduler\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y.unsqueeze(1)) # Ensure target y has same shape as output\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs # data.num_graphs gives the number of graphs in the batch\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad() # Decorator for no gradient calculation during evaluation\n",
    "def evaluate_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y.unsqueeze(1))\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        all_preds.append(out.cpu())\n",
    "        all_targets.append(data.y.unsqueeze(1).cpu())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    # You can also calculate other metrics like R2, MAE here\n",
    "    # preds_tensor = torch.cat(all_preds, dim=0)\n",
    "    # targets_tensor = torch.cat(all_targets, dim=0)\n",
    "    # r2 = r2_score(targets_tensor.numpy(), preds_tensor.numpy()) # from sklearn.metrics\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "num_epochs = 100 # Example: increase for real training\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate_epoch(model, val_loader, criterion)\n",
    "    scheduler.step(val_loss) # Step scheduler based on validation loss\n",
    "\n",
    "    print(f\"Epoch {epoch:03d}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_gnn_model.pth')\n",
    "        print(f\"Saved new best model with Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2208bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3692708/3052161693.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_gnn_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation:\n",
      "Test MSE Loss: 68.1813\n",
      "Test RMSE: 8.2572\n",
      "Test R-squared: 0.6298\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_gnn_model.pth'))\n",
    "model = model.to(device) # Ensure model is on the correct device\n",
    "\n",
    "test_loss = evaluate_epoch(model, test_loader, criterion)\n",
    "print(f\"\\nTest Set Evaluation:\")\n",
    "print(f\"Test MSE Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Get predictions and true values for more detailed metrics\n",
    "model.eval()\n",
    "all_preds_test = []\n",
    "all_targets_test = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        all_preds_test.append(out.cpu())\n",
    "        all_targets_test.append(data.y.unsqueeze(1).cpu())\n",
    "\n",
    "preds_test_tensor = torch.cat(all_preds_test, dim=0).squeeze().numpy()\n",
    "targets_test_tensor = torch.cat(all_targets_test, dim=0).squeeze().numpy()\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_test = np.sqrt(mean_squared_error(targets_test_tensor, preds_test_tensor))\n",
    "r2_test = r2_score(targets_test_tensor, preds_test_tensor)\n",
    "\n",
    "print(f\"Test RMSE: {rmse_test:.4f}\")\n",
    "print(f\"Test R-squared: {r2_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
